{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries"
      ],
      "metadata": {
        "id": "Or_rOPzHSEfu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woXUeHbpRlqf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D,\n",
        "Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,\n",
        "precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section imports all the necessary libraries and modules required for building\n",
        "and evaluating the deep learning model. It includes libraries for numerical computations\n",
        "(numpy), data visualization (matplotlib), deep learning (TensorFlow and Keras), and evaluation\n",
        "metrics (scikit-learn)."
      ],
      "metadata": {
        "id": "ZOUY4TYwSOzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading and Preprocessing the Dataset"
      ],
      "metadata": {
        "id": "nqlaVjn-SH_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Cifar-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# Use a smaller subset of the data for quick testing\n",
        "x_train_small, y_train_small = x_train[:1000], y_train[:1000]\n",
        "x_test_small, y_test_small = x_test[:200], y_test[:200]\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train_small = x_train_small.astype('float32') / 255.0\n",
        "x_test_small = x_test_small.astype('float32') / 255.0\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "y_train_small = to_categorical(y_train_small, 10)\n",
        "y_test_small = to_categorical(y_test_small, 10)"
      ],
      "metadata": {
        "id": "i-6h2oe5R1q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section loads the CIFAR-10 dataset, which contains 60,000 32x32 color\n",
        "images in 10 different classes. For quick testing, smaller subsets of the training and test data\n",
        "are created. The images are normalized by scaling pixel values to the range [0, 1], and the\n",
        "labels are converted to one-hot encoding format."
      ],
      "metadata": {
        "id": "_ezpzlweSTPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building the Model"
      ],
      "metadata": {
        "id": "3DmGFvMXSW8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model function\n",
        "def build_model(optimizer, loss):\n",
        "model = Sequential([\n",
        "Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "MaxPooling2D((2, 2)),\n",
        "Conv2D(64, (3, 3), activation='relu'),\n",
        "MaxPooling2D((2, 2)),\n",
        "Conv2D(128, (3, 3), activation='relu'),\n",
        "Flatten(),\n",
        "Dense(512, activation='relu'),\n",
        "Dropout(0.5),\n",
        "Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer=optimizer, loss=loss,\n",
        "metrics=['accuracy'])\n",
        "return model\n"
      ],
      "metadata": {
        "id": "m91w7TIkSTso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines a function build_model that constructs a Convolutional\n",
        "Neural Network (CNN) using the Keras Sequential API. The model consists of convolutional\n",
        "layers, max-pooling layers, a dense layer with dropout for regularization, and a final output layer\n",
        "with softmax activation. The model is compiled with the specified optimizer and loss function."
      ],
      "metadata": {
        "id": "mIU_USpWS9dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training and Evaluation"
      ],
      "metadata": {
        "id": "yLLOtVCPTDXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize optimizers and loss functions\n",
        "optimizers = ['adam', 'sgd', 'rmsprop']\n",
        "loss_functions = ['categorical_crossentropy', 'mean_squared_error']\n",
        "\n",
        "# Clear previous history from memory to avoid confusion\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Function to evaluate the model and compute metrics\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "specificity = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
        "return conf_matrix, accuracy, precision, recall, f1, specificity\n",
        "history_list = []\n",
        "metrics_list = []\n",
        "\n",
        "for opt_name in optimizers:\n",
        "for loss_name in loss_functions:\n",
        "print(f\"\\nTraining with Optimizer: {opt_name}, Loss Function: {loss_name}\\n\")\n",
        "\n",
        "if opt_name == 'adam':\n",
        "optimizer = tf.keras.optimizers.legacy.Adam()\n",
        "elif opt_name == 'sgd':\n",
        "optimizer = tf.keras.optimizers.legacy.SGD()\n",
        "elif opt_name == 'rmsprop':\n",
        "optimizer = tf.keras.optimizers.legacy.RMSprop()\n",
        "model = build_model(optimizer, loss_name)\n",
        "\n",
        "history = model.fit(x_train_small, y_train_small, epochs=2,\n",
        "validation_data=(x_test_small, y_test_small), batch_size=64,\n",
        "verbose=2)\n",
        "history_list.append((opt_name, loss_name, history))\n",
        "\n",
        "# Evaluate model and collect metrics\n",
        "conf_matrix, accuracy, precision, recall, f1, specificity =\n",
        "evaluate_model(model, x_test_small, y_test_small)\n",
        "metrics_list.append((opt_name, loss_name, conf_matrix,\n",
        "accuracy, precision, recall, f1, specificity))\n",
        "\n",
        "# Clear session to avoid conflicts in subsequent trainings\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "cqOjxGcyTACc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section initializes a list of optimizers and loss functions to experiment with. It\n",
        "defines a function `evaluate_model` to compute and return various evaluation metrics. The\n",
        "models are trained and evaluated using different combinations of optimizers and loss functions.\n",
        "The metrics for each combination are stored in `metrics_list`."
      ],
      "metadata": {
        "id": "Tm-EygNvTdzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying Metrics and Saving the Best Model"
      ],
      "metadata": {
        "id": "fA9TrkK2Tvxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying metrics\n",
        "for opt_name, loss_name, conf_matrix, accuracy, precision, recall, f1,\n",
        "specificity in metrics_list:\n",
        "print(f\"Optimizer: {opt_name}, Loss Function: {loss_name}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Specificity: {specificity}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Save the best model's weights for testing\n",
        "model.save_weights('best_model_weights.h5')"
      ],
      "metadata": {
        "id": "uluOs354Tswn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section displays the evaluation metrics for each combination of optimizer and\n",
        "loss function. It also saves the weights of the best-performing model for later use.\n"
      ],
      "metadata": {
        "id": "0m8WoyntT8wN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Image Prediction"
      ],
      "metadata": {
        "id": "7P6MmIm6UBMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the image\n",
        "def load_and_preprocess_image(img_path):\n",
        "img = image.load_img(img_path, target_size=(32, 32))\n",
        "img_array = image.img_to_array(img) / 255.0\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "return img_array\n",
        "\n",
        "# Load the best model\n",
        "best_model = build_model(tf.keras.optimizers.legacy.Adam(),\n",
        "'categorical_crossentropy')\n",
        "best_model.load_weights('best_model_weights.h5')\n",
        "\n",
        "# Load your custom image\n",
        "img_path = '/content/bird.jpg' # Replace with your image path\n",
        "img_array = load_and_preprocess_image(img_path)\n",
        "\n",
        "# Predict the class of the custom image\n",
        "prediction = best_model.predict(img_array)\n",
        "predicted_class = np.argmax(prediction)\n",
        "\n",
        "# Map class index to class name\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "predicted_object = class_names[predicted_class]\n",
        "print(f\"Predicted object: {predicted_object}\")\n"
      ],
      "metadata": {
        "id": "OXESg8j8Tz1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines a function to preprocess custom images for prediction. It then\n",
        "loads the best model, preprocesses a custom image, and predicts its class. The predicted class\n",
        "index is mapped to the corresponding class name and printed."
      ],
      "metadata": {
        "id": "CXsq6LxZUPjb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8eGc3m3aUQJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}